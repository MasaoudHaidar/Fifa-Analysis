{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "Import the necessary library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import warnings\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data\n",
    "\n",
    "Reading the data for all the players. The data is read in Dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player15_df = pd.read_csv('./data/players_15.csv')\n",
    "player16_df = pd.read_csv('./data/players_16.csv')\n",
    "player17_df = pd.read_csv('./data/players_17.csv')\n",
    "player18_df = pd.read_csv('./data/players_18.csv')\n",
    "player19_df = pd.read_csv('./data/players_19.csv')\n",
    "player20_df = pd.read_csv('./data/players_20.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleanup\n",
    "\n",
    "The purpose of the cells below is to make sure that we drop the columns that have been identified as not to be used.\n",
    "\n",
    "In addition to the columns indicated in the statement, the team also feels that the clubs staff capabilities do not depend on other attributes like:\n",
    "1. Player Height\n",
    "1. Player Weight\n",
    "1. Player Nationality\n",
    "\n",
    "So these columns have been also identified as to be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_player_df(player_df):\n",
    "    '''\n",
    "    Function below takes the Dataframe and drops the columns which are specified in the list above.\n",
    "    '''\n",
    "    return player_df.drop(columns_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning up all the dataframes to remove the columns identified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "For the purpose of Data Analysis, identifying columns which are numerical would simplify the quantitative analysis of the stafs capabilities. In order to identify those columns and run some preliminary analysis like min values max values, identifying the columns which are numeric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Identify Numerical Columns\n",
    "\n",
    "#### Inference from the step:\n",
    "The list above indicates that there are about 24 columns which are numerical and can be leveraged for the sake of quantitative analysis. \n",
    "However further investigation of these numerical columns indicate that some of these columns could be added to the cleanup of columns as they would not really reflect the staffs capabilities to promote talent.\n",
    "\n",
    "#### Inference Action:\n",
    "Add the columns to the list of columns to be cleaned up and remove the columns.\n",
    "\n",
    "Columns identified:\n",
    "1. value_eur\n",
    "1. release_clause_eur\n",
    "1. team_jersey_number\n",
    "1. contract_valid_until\n",
    "1. nation_jersey_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"sofifa_id\", \"player_url\", \"long_name\", \"wage_eur\", \"real_face\", \"height_cm\", \"weight_kg\", \"nationality\", \n",
    "                    \"value_eur\", \"release_clause_eur\", \"team_jersey_number\", \"contract_valid_until\",\"nation_jersey_number\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further cleanup\n",
    "player15_cleaned_df = clean_player_df(player15_df)\n",
    "player16_cleaned_df = clean_player_df(player16_df)\n",
    "player17_cleaned_df = clean_player_df(player17_df)\n",
    "player18_cleaned_df = clean_player_df(player18_df)\n",
    "player19_cleaned_df = clean_player_df(player19_df)\n",
    "player20_cleaned_df = clean_player_df(player20_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player15_cleaned_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Analyze the Numerical Columns\n",
    "\n",
    "In order to check the quality of data available in those numerical columns, analyzing a couple of years to see the type and quality of data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inferences:\n",
    "1. The Describe on the multiple years does indicate that the mental_composure is not a column that we can rely on as the values are not captured in the earlier years and further investigation indicated that the data has a format which is a split value (e.g. and hence can be excluded 90+3)\n",
    "1. International reputation is also a Categorical variable with values 1 to 5.\n",
    "1. Weak Foot is a categorical value too with values 1 to 5.\n",
    "1. Numerical columns 'gk_speed', 'gk_handling', 'gk_kicking', 'gk_positioning', 'gk_diving' have very few values. On inspecting the players associated with those values, it was identified that these are goal keepers and attributes for goal keepers. As the question is about effectiveness of the entire staff, the approach was to drop these columns as well both for lack of data and to avoid focus on skill development for goal keepers.\n",
    "\n",
    "#### Inference Action\n",
    "1. Remove the mentality_composure value from the columns. \n",
    "2. From domain knowledge perspective, weak foot can be eliminated as other factors would reflect if the staff has improved the weak foot score of the individual.\n",
    "3. International reputation is also being dropped because we are not measuring PR teams capabilities but the teams staff.\n",
    "~~\n",
    "4. Remove the gk_* values from the columns list.  \n",
    "5. Also as the Goal Keepers are missing the information of other attributes, we might not be able to get metrics on the staffs work on the goal keepers. \n",
    "~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = [\"player_url\", \"long_name\", \"wage_eur\", \"real_face\", \"height_cm\", \"weight_kg\", \"nationality\", \n",
    "                    \"value_eur\", \"release_clause_eur\", \"team_jersey_number\", \"contract_valid_until\",\"nation_jersey_number\",\n",
    "                      \"mentality_composure\", \"weak_foot\", \"international_reputation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reloading the data so that we can reclean\n",
    "player15_df = pd.read_csv('./data/players_15.csv')\n",
    "player16_df = pd.read_csv('./data/players_16.csv')\n",
    "player17_df = pd.read_csv('./data/players_17.csv')\n",
    "player18_df = pd.read_csv('./data/players_18.csv')\n",
    "player19_df = pd.read_csv('./data/players_19.csv')\n",
    "player20_df = pd.read_csv('./data/players_20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further cleanup\n",
    "player15_cleaned_df = clean_player_df(player15_df)\n",
    "player16_cleaned_df = clean_player_df(player16_df)\n",
    "player17_cleaned_df = clean_player_df(player17_df)\n",
    "player18_cleaned_df = clean_player_df(player18_df)\n",
    "player19_cleaned_df = clean_player_df(player19_df)\n",
    "player20_cleaned_df = clean_player_df(player20_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skills Columns\n",
    "\n",
    "The skills columns in the dataframe indicate the different skills with a numerical value for the Skills. The team thinks that the skills are an important part of the development of every player. The player skills once improved will contribute to the overall improvement of the player and hence its overall rating. \n",
    "\n",
    "#### Observations:\n",
    "1. Non Numeric columns: \n",
    "The columns for skills are multiple and are non numeric. Infact the columns have values with + and - signs indicating that the columns have data which indicates positive and negative traits.\n",
    "\n",
    "#### Observation Action:\n",
    "\n",
    "1. The decision is to exclude the columns for the first set of determination and keep only the numeric columns in the list. \n",
    "2. Build a data frame for every year with only these numeric values in addition to columns which identify the player like name, club.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n",
    "numeric_column_list = list(player15_cleaned_df.select_dtypes(include=numerics).columns)\n",
    "print (pd.DataFrame(numeric_column_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Numerical Only Dataset\n",
    "\n",
    "Creating a new Dataset with only numerical columns:\n",
    "0          age\n",
    "1      overall\n",
    "2    potential\n",
    "3  skill_moves\n",
    "4         pace\n",
    "5     shooting\n",
    "6      passing\n",
    "7    dribbling\n",
    "8    defending\n",
    "9       physic\n",
    "10 short_name\n",
    "11 club\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# player_trait_columns = [\"overall\", \"pace\" \"shooting\",\"passing\",\"dribbling\" \"defending\"]\n",
    "\n",
    "player_numeric_identification_columns = ['short_name', \"club\"] +  numeric_column_list\n",
    "print (player_numeric_identification_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player15_numeric_cleaned_df =  player15_cleaned_df[player_numeric_identification_columns]\n",
    "player16_numeric_cleaned_df =  player16_cleaned_df[player_numeric_identification_columns]\n",
    "player17_numeric_cleaned_df =  player17_cleaned_df[player_numeric_identification_columns]\n",
    "player18_numeric_cleaned_df =  player18_cleaned_df[player_numeric_identification_columns]\n",
    "player19_numeric_cleaned_df =  player19_cleaned_df[player_numeric_identification_columns]\n",
    "player20_numeric_cleaned_df =  player20_cleaned_df[player_numeric_identification_columns]\n",
    "numeric_column_list.remove(\"sofifa_id\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Year Over Year Comparison\n",
    "\n",
    "#### Approach\n",
    "Since the purpose of the exercise is to identify the effectiveness of the staff, the approach is to identify the score differences of the players year over year. \n",
    "\n",
    "We are going to join data year over year and see the score differences. The approach is to create a dataframe that would have the name of the player the club and the difference in the rating of the numeric column for the player.\n",
    "\n",
    "#### Decisions:\n",
    "1. Initially the team considered only players who stayed at the club to get a guage of the players skill. However given that not many players were staying at the club, the decision was to not to enforce the stay at club metric. The credit for the increase is being assigned to the club in the earlier year.\n",
    "1. The score difference will be calculated for every numeric parameter including the values which are categorical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a year over year dataframe in which the values will be stored\n",
    "year_over_year_df = pd.DataFrame()\n",
    "all_years_df = pd.DataFrame()\n",
    "# year_over_year_df = player15_numeric_cleaned_df[\"short_name\"]\n",
    "# display(year_over_year_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_year_over_year_df(year1_df, year2_df, year):\n",
    "    year1_year2_joined_df = year1_df.merge(year2_df, on=\"sofifa_id\", suffixes=('_1', '_2'))\n",
    "    year_over_year_df[\"short_name\"] = year1_year2_joined_df[\"short_name_1\"]\n",
    "    year_over_year_df[\"club\"] = year1_year2_joined_df[\"club_1\"]\n",
    "    year_over_year_df[\"age\"] = year1_year2_joined_df[\"age_1\"]    \n",
    "    year_over_year_df[\"year_over_year\"] = year\n",
    "    for column in numeric_column_list:\n",
    "        year_over_year_df[f\"diff_{column}\"] = year1_year2_joined_df[f\"{column}_2\"] - year1_year2_joined_df[f\"{column}_1\"]\n",
    "        year_over_year_df[column] = year1_year2_joined_df[[f\"{column}_1\"]]         \n",
    "    return year_over_year_df\n",
    "        \n",
    "all_years_df = all_years_df.append(create_year_over_year_df(player15_numeric_cleaned_df, player16_numeric_cleaned_df, 2016))\n",
    "all_years_df = all_years_df.append(create_year_over_year_df(player16_numeric_cleaned_df, player17_numeric_cleaned_df, 2017)) \n",
    "all_years_df = all_years_df.append(create_year_over_year_df(player17_numeric_cleaned_df, player18_numeric_cleaned_df, 2018))\n",
    "all_years_df = all_years_df.append(create_year_over_year_df(player18_numeric_cleaned_df, player19_numeric_cleaned_df, 2019)) \n",
    "all_years_df = all_years_df.append(create_year_over_year_df(player19_numeric_cleaned_df, player20_numeric_cleaned_df, 2020)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_df.head()\n",
    "numeric_column_list = list(player15_cleaned_df.select_dtypes(include=numerics).columns)\n",
    "for column in numeric_column_list: \n",
    "    if (column != 'sofifa_id'):\n",
    "        all_years_df[f\"diff_{column}\"].fillna(0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Difference Matrix\n",
    "\n",
    "The year over year dataframe has all the score changes for the Club for every individual player that has played for the club.\n",
    "\n",
    "#### Approach 1\n",
    "Find the number of players who have a positive change for every metric. This would mean that we create a histogram and find how many players have a positive change. We are going to only count players who have atleast one standard deviation change of positive change in rating for every metric. That will give us indication of how important is this metric in staffs contribution.\n",
    "##### Decision\"\n",
    "Instead of only using the Players with positive score we decided to use both negative and positive scores so the clubs which are performing bad could be penalized.\n",
    "\n",
    "#### Approach 2\n",
    "For every club find the average change in ratings for each of the metrics and then order the clubs to identify which clubs have maximum change and order those clubs. This will ensure that we not only count players which are improving but also players which are losing points.\n",
    "Plot the top 10 clubs that have shown the most change.\n",
    "\n",
    "#### Approach 3\n",
    "Define a Linear Regression model to determine what should be the change in the overall rating for a player at an age.\n",
    "\n",
    "Find the change in the Players metrics. Based upon the metrics find the overall score change that is desired based upon the individual factors using a Linear Regression model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation:\n",
    "\n",
    "From the graphs above we observe:\n",
    "1. Players of higher ages tend to show less changes in all the metrics. Infact playersa above the age between 30 and 35 tend to show lesser changes in the skill area improvements. \n",
    "2. Younger players do tend to show higher changes in the skill profile values.\n",
    "3. Certain higher values for differences might skew the data and might have to be removed.\n",
    "\n",
    "#### Actions:\n",
    "1. Age definitely plays a role in the calculation of the score changes and hence we are going to calculate the average on every age.\n",
    "1. Average the score changes for a club at every age and identify the clubs which are performing better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_df = all_years_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_years_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_over_year_metrics_averaged = all_years_df\n",
    "year_over_year_metrics_averaged = year_over_year_metrics_averaged.drop([\"short_name\"], axis=1)\n",
    "year_over_year_metrics_averaged = year_over_year_metrics_averaged.groupby([\"club\", \"year_over_year\"]).sum().reset_index()\n",
    "year_over_year_metrics_averaged = year_over_year_metrics_averaged.dropna()\n",
    "\n",
    "display (year_over_year_metrics_averaged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding Clubs which have done better in all age group\n",
    "sortable_columns = []\n",
    "for column in all_years_df.columns:\n",
    "    if (\"diff_\" in column and column != \"diff_age\"):\n",
    "        sortable_columns.append(column)\n",
    "best_clubs_any_year = year_over_year_metrics_averaged.sort_values(by=sortable_columns, ascending = False).head(10)\n",
    "display (best_clubs_any_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test and Train Set\n",
    "\n",
    "The problem states that we are using the data from the Division 1 European League. In order to make that happen for the data that we have cleaned up, we are going to separate the data for the clubs into Train set belonging to the players from the league.\n",
    "\n",
    "#### Steps:\n",
    "1. Investigation of the dataset has indicated that the League information is not in the data file and needs to be fetched from the web using the scraping approach.\n",
    "2. Once the scraping pulls data the data is going to be appended to the club score changes dataframe. \n",
    "\n",
    "#### Model Options\n",
    "1. Linear Model using fixed values based upon the score changes.\n",
    "2. Linear Model but instead of using fixed values use the coefficients of the linear equation that generates the overall score and then assign the score based upon the coefficient values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Approach:\n",
    "1. Read the Club Data and the Leagues data files\n",
    "2. Match the club and the leagues in which the clubs play\n",
    "3. Create a test data set and a train dataset by excluding the the clubs that are associated with the Leagues marked as Test data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web Scraping\n",
    "\n",
    "The leagues and teams dataset does not have any correlation with the clubs that play in a league and hence we need to scrape that data from the sofifa site.\n",
    "The url field in the dataset can be used to make a web request and then download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def parseTeamNameFromUrl(team_url_id):\n",
    "\n",
    "    source = requests.get(f\"https://sofifa.com/team/{team_url_id}\")\n",
    "    soup = bs.BeautifulSoup(source.content)\n",
    "    team_info_divs = soup.findAll(\"div\", {\"class\": \"info\"})\n",
    "    team_name = 'not found'\n",
    "    for div in team_info_divs:\n",
    "        team_name = div.find(\"h1\").text \n",
    "    print (team_name)\n",
    "    return team_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parseTeamNameFromUrl(10030)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Web Scraping End\n",
    "\n",
    "Once the web scraping is done we do not intend to save the file and no need to keep running the scraping every execution. This is the lambda based approach to make a call to every row and pull the data. this is not done everytime and I have pulled that data down to my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leagues_with_club_df = pd.read_csv('./data/teams_leagues_clubs.csv')\n",
    "display(leagues_with_club_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leagues_df[\"club\"] = leagues_df.apply(lambda x: parseTeamNameFromUrl(x['url']),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leagues_df.to_csv('./data/teams_leagues_clubs.csv', index=False)\n",
    "# display(leagues_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_and_leagues = pd.read_csv('./data/teams_and_leagues.csv')\n",
    "display(teams_and_leagues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the league information into the dataframe to be able to sort the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_over_year_metrics_averaged_with_leagues = year_over_year_metrics_averaged.merge(leagues_with_club_df, on=\"club\", suffixes=('_1', '_2'))\n",
    "display(year_over_year_metrics_averaged_with_leagues[[\"club\", \"year_over_year\", \"overall\", \"diff_overall\", \"skill_moves\", \"diff_skill_moves\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Scoring\n",
    "year_over_year_metrics_averaged_with_leagues_aggregated_over_all_years = year_over_year_metrics_averaged_with_leagues.groupby([\"club\"]).sum().reset_index()\n",
    "year_over_year_metrics_averaged_with_leagues_aggregated_over_all_years"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since we are splitting based upon data points and not a percentage, this method would get the x and y train sets\n",
    "def train_test_split_custom():\n",
    "    club_test_set = year_over_year_metrics_averaged_with_leagues[year_over_year_metrics_averaged_with_leagues['league_name'].isin([\"English Premier League \", \"German 1. Bundesliga \", \"French Ligue 1 \", \"Spain Primera Division \", \"Italian Serie A \"])]\n",
    "\n",
    "    club_train_set = year_over_year_metrics_averaged_with_leagues[~year_over_year_metrics_averaged_with_leagues['league_name'].isin([\"English Premier League \", \"German 1. Bundesliga \", \"French Ligue 1 \", \"Spain Primera Division \", \"Italian Serie A \"])]\n",
    "\n",
    "#     x_train = club_train_set.drop([\"club\",\"year_over_year\",\"age\",\"diff_age\",\"diff_overall\",\"league_name\"], axis=1)\n",
    "#     y_train = club_train_set[\"diff_overall\"]\n",
    "\n",
    "#     x_test = club_test_set.drop([\"club\",\"year_over_year\",\"age\",\"diff_age\",\"diff_overall\",\"league_name\"], axis=1)\n",
    "#     y_test = club_test_set[\"diff_overall\"]\n",
    "    \n",
    "    x_train = club_train_set.drop([\"club\",\"diff_age\",\"diff_overall\",\"league_name\", \"url\"], axis=1)\n",
    "    y_train = club_train_set[\"diff_overall\"]\n",
    "\n",
    "    x_test = club_test_set.drop([\"club\",\"diff_age\",\"diff_overall\",\"league_name\", \"url\"], axis=1)\n",
    "    y_test = club_test_set[\"diff_overall\"]    \n",
    "    \n",
    "    return club_train_set, club_test_set, x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scored_df( train_df, test_df):\n",
    "    \n",
    "    club_test_set_results = test_df.copy()\n",
    "    # Difference between the model times the diff overall\n",
    "    # club_test_set_results[\"score\"] = club_test_set_results[\"diff_overall\"] *  (club_test_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"]) \n",
    "    # Product of predicted versus overall\n",
    "    # club_test_set_results[\"score\"] = club_test_set_results[\"diff_overall\"] * club_test_set_results[\"predicted\"] \n",
    "    # Difference between predicted versus overall\n",
    "    # club_test_set_results[\"score\"] = (club_test_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"]) \n",
    "    # Difference between the model and sum of diff overall\n",
    "    club_test_set_results[\"score\"] = club_test_set_results[\"diff_overall\"] + (club_test_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"]) \n",
    "    club_test_set_results[\"score_raw\"] = club_test_set_results[\"score\"]\n",
    "    club_test_set_results = club_test_set_results.drop([\"year_over_year\", \"diff_age\"], axis=1)    \n",
    "    club_test_set_results = club_test_set_results.groupby([\"club\"]).mean().reset_index()\n",
    "#     cols_to_norm = ['score']\n",
    "#     club_test_set_results[cols_to_norm] = club_test_set_results[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "#     club_test_set_results[cols_to_norm] = club_test_set_results[cols_to_norm] * 100\n",
    "    \n",
    "    club_test_set_results[['score']] = MinMaxScaler().fit_transform(club_test_set_results[['score']])\n",
    "    club_test_set_results['score'] = club_test_set_results['score'] * 100\n",
    "    \n",
    "    club_train_set_results = train_df.copy()\n",
    "    # Difference between the model times the diff overall\n",
    "    # club_train_set_results[\"score\"] = club_train_set_results[\"diff_overall\"] *  (club_train_set_results[\"diff_overall\"] - club_train_set_results[\"predicted\"])\n",
    "    # Difference between the model times the diff overall\n",
    "    club_train_set_results[\"score\"] = club_train_set_results[\"diff_overall\"] +  (club_train_set_results[\"diff_overall\"] - club_train_set_results[\"predicted\"])    \n",
    "    # Product of predicted versus overall\n",
    "    # club_train_set_results[\"score\"] = club_train_set_results[\"diff_overall\"] * club_train_set_results[\"predicted\"] \n",
    "    # Difference between predicted versus overall\n",
    "    # club_train_set_results[\"score\"] = (club_train_set_results[\"diff_overall\"] - club_train_set_results[\"predicted\"])    \n",
    "    club_train_set_results[\"score_raw\"] = club_train_set_results[\"score\"]\n",
    "\n",
    "    club_train_set_results = club_train_set_results.drop([\"year_over_year\", \"diff_age\"], axis=1)\n",
    "    club_train_set_results = club_train_set_results.groupby([\"club\"]).mean().reset_index()\n",
    "#     cols_to_norm = ['score']\n",
    "#     club_train_set_results[cols_to_norm] = club_train_set_results[cols_to_norm].apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "#     club_train_set_results[cols_to_norm] = club_train_set_results[cols_to_norm] * 100\n",
    "    club_train_set_results[['score']] = MinMaxScaler().fit_transform(club_train_set_results[['score']])\n",
    "    club_train_set_results['score'] = club_train_set_results['score'] * 100\n",
    "\n",
    "    \n",
    "    return club_train_set_results, club_test_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring Approach\n",
    "\n",
    "The function here enumerates the scoring approaches and provides a method to select the approach. Default method is to use the difference overall + (difference overall - predicted difference overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class ScoreApproach(Enum):\n",
    "    DIFF_OVERALL_TIMES_DIFFERENCE = 1\n",
    "    DIFF_OVERALL_TIMES_PREDICTED = 2\n",
    "    DIFF_OVERALL_PLUS_DIFFERENCE = 3\n",
    "    OVERALL_PLUS_DIFF_OVERALL_PLUS_DIFFERENCE = 4\n",
    "    DIFF_OVERALL_PREDICTED = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_scored_with_approach_selection_df( train_df, test_df, score_approach=ScoreApproach.DIFF_OVERALL_PLUS_DIFFERENCE):\n",
    "    \n",
    "    club_test_set_results = test_df.copy()\n",
    "    if (score_approach == ScoreApproach.DIFF_OVERALL_TIMES_DIFFERENCE):\n",
    "        # Difference between the model times the diff overall\n",
    "        club_test_set_results[\"score\"] = club_test_set_results[\"diff_overall\"] *  (club_test_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"]) \n",
    "    elif (score_approach == ScoreApproach.DIFF_OVERALL_TIMES_PREDICTED):\n",
    "        # Product of predicted versus overall\n",
    "        club_test_set_results[\"score\"] = club_test_set_results[\"diff_overall\"] * club_test_set_results[\"predicted\"] \n",
    "    elif (score_approach == ScoreApproach.DIFF_OVERALL_PLUS_DIFFERENCE):\n",
    "        # Difference between the model and sum of diff overall\n",
    "        club_test_set_results[\"score\"] = club_test_set_results[\"diff_overall\"] +  (club_test_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"]) \n",
    "    elif (score_approach == ScoreApproach.OVERALL_PLUS_DIFF_OVERALL_PLUS_DIFFERENCE):\n",
    "        # Difference between the model and sum of diff overall\n",
    "        club_test_set_results[\"score\"] = club_test_set_results[\"overall\"] + club_test_set_results[\"diff_overall\"] +  (club_test_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"])         \n",
    "    elif (score_approach == ScoreApproach.DIFF_OVERALL_PREDICTED):\n",
    "        # Difference between the model and sum of diff overall\n",
    "        club_test_set_results[\"score\"] = (club_test_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"])         \n",
    "\n",
    "    club_test_set_results[\"score_raw\"] = club_test_set_results[\"score\"]\n",
    "    club_test_set_results = club_test_set_results.drop([\"year_over_year\", \"diff_age\"], axis=1)    \n",
    "    club_test_set_results = club_test_set_results.groupby([\"club\"]).mean().reset_index()\n",
    "    # Normalize between 1 and 100\n",
    "    club_test_set_results[['score']] = MinMaxScaler().fit_transform(club_test_set_results[['score']])\n",
    "    club_test_set_results['score'] = club_test_set_results['score'] * 100\n",
    "    \n",
    "    club_train_set_results = train_df.copy()\n",
    "    if (score_approach == ScoreApproach.DIFF_OVERALL_TIMES_DIFFERENCE):\n",
    "        # Difference between the model times the diff overall\n",
    "        club_train_set_results[\"score\"] = club_train_set_results[\"diff_overall\"] *  (club_train_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"]) \n",
    "    elif (score_approach == ScoreApproach.DIFF_OVERALL_TIMES_PREDICTED):\n",
    "        # Product of predicted versus overall\n",
    "        club_train_set_results[\"score\"] = club_train_set_results[\"diff_overall\"] * club_train_set_results[\"predicted\"] \n",
    "    elif (score_approach == ScoreApproach.DIFF_OVERALL_PLUS_DIFFERENCE):\n",
    "        # Difference between the model and sum of diff overall\n",
    "        club_train_set_results[\"score\"] = club_train_set_results[\"diff_overall\"] +  (club_train_set_results[\"diff_overall\"] - club_test_set_results[\"predicted\"]) \n",
    "    elif (score_approach == ScoreApproach.OVERALL_PLUS_DIFF_OVERALL_PLUS_DIFFERENCE):\n",
    "        # Difference between the model and sum of overall\n",
    "        club_train_set_results[\"score\"] = club_train_set_results[\"overall\"] + club_train_set_results[\"diff_overall\"] +  (club_train_set_results[\"diff_overall\"] - club_train_set_results[\"predicted\"])         \n",
    "    elif (score_approach == ScoreApproach.DIFF_OVERALL_PREDICTED):\n",
    "        # Difference between the model and sum of diff overall\n",
    "        club_train_set_results[\"score\"] = (club_train_set_results[\"diff_overall\"] - club_train_set_results[\"predicted\"])  \n",
    "        \n",
    "    club_train_set_results[\"score_raw\"] = club_train_set_results[\"score\"]\n",
    "\n",
    "    club_train_set_results = club_train_set_results.drop([\"year_over_year\", \"diff_age\"], axis=1)\n",
    "    club_train_set_results = club_train_set_results.groupby([\"club\"]).mean().reset_index()\n",
    "    # Normalize between 1 to 100\n",
    "    club_train_set_results[['score']] = MinMaxScaler().fit_transform(club_train_set_results[['score']])\n",
    "    club_train_set_results['score'] = club_train_set_results['score'] * 100\n",
    "    print(club_train_set_results.shape)\n",
    "    print(club_test_set_results.shape)\n",
    "    return club_train_set_results, club_test_set_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Analysis and Visualization Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def displayModelTestScoreAgeScatter(club_test_set_model):\n",
    "    plt.scatter(club_test_set_model[\"age\"], club_test_set_model[\"score\"])\n",
    "    plt.xlabel(\"Age\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.title(\"Club Scores at different Ages\")\n",
    "    plt.show()\n",
    "\n",
    "def displayModelTestScoreClubHistogram(club_test_set_model):\n",
    "    plt.hist(club_test_set_model[\"score\"], bins=20)\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Number of Clubs\")\n",
    "    plt.title(\"Histogram showing number of clubs \")\n",
    "    plt.show()\n",
    "    \n",
    "def displayModelTestScoreClubHistogramByLeague(club_test_set_model):\n",
    "    league_names = club_test_set_model.league_name.unique()\n",
    "    league_scores = []\n",
    "    for league_name in league_names:\n",
    "        league_scores.append(club_test_set_model[club_test_set_model[\"league_name\"] == league_name][\"score\"].values)\n",
    "\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.hist(league_scores, bins = 10, histtype='bar', label=league_names)\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Number of Clubs\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def displayTop10ClubsForEachAge(club_test_set_model):\n",
    "\n",
    "#     fig, axs = plt.subplots(5,3,figsize=(32,16))\n",
    "#     collabel=(\"Club\", \"Score\")\n",
    "#     i = 0\n",
    "#     j = 0\n",
    "#     for i in [0,1,2,3,4]: \n",
    "#         for j in [0,1,2]:\n",
    "#             axs[i][j].axis('tight')\n",
    "#             axs[i][j].axis('off')\n",
    "#             age = i*3+j+16\n",
    "#             club_test_data_for_age = club_test_set_model[club_test_set_model[\"age\"] == age].sort_values(by=\"score\", ascending = False).head(10)\n",
    "#             outof = len(club_test_set_model[club_test_set_model[\"age\"] == age])\n",
    "#             result = club_test_data_for_age[[\"club\",\"score\"]]\n",
    "#             result['score'] = result['score'].map('{:.4f}'.format)\n",
    "#             axs[i][j].table(cellText=result.values, colLabels=collabel, loc='center',colWidths=[0.3 for x in collabel])\n",
    "#             axs[i][j].set_title(f\"Clubs identified as top {len(club_test_data_for_age)} out of {outof} for Age - {age} \")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def displayBestClubsOverAllAges(club_test_set_model, top_n = 10):\n",
    "    # Finding the Clubs that performed best overall at all ages\n",
    "    club_test_set_model = club_test_set_model.drop_duplicates()\n",
    "#     all_ages_club_df = pd.DataFrame()\n",
    "#     for age in range(16, 41):\n",
    "#         club_test_data_for_age = club_test_set_model[club_test_set_model[\"age\"] == age].sort_values(by=\"score\", ascending = False).head(10)\n",
    "#         all_ages_club_df=all_ages_club_df.append(club_test_set_model, ignore_index=True)\n",
    "\n",
    "#     all_ages_club_mean_scores_df = club_test_set_model.groupby(by=[\"club\"]).mean()[[\"score\"]]\n",
    "    all_ages_club_mean_scores_df = club_test_set_model[[\"club\", \"diff_overall\", \"predicted\",  \"score_raw\",  \"score\", \"url\"]]\n",
    "\n",
    "    result_df = all_ages_club_mean_scores_df.sort_values(by=\"score\", ascending=False)\n",
    "    print (f\"Top {top_n} Clubs\")\n",
    "    display(result_df.head(top_n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Models\n",
    "\n",
    "The Scoring Approach is to train a model on the difference of the overall score of the club for all the parameters. Once the model is trained predict the overall score improvements anticipated from the other factors for the club. If the overall predicted score is greater then give the team a positive score else give the team a negative score. Normalize the scores on a scale of 100. In order to predict the scores we have used 3 models - Linear, Linear with Ridge and Random Forests\n",
    "\n",
    "### Linear Regression Model\n",
    "\n",
    "#### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set, club_test_set, x_train, y_train, x_test, y_test = train_test_split_custom()\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x_train, y_train)\n",
    "\n",
    "y_train_pred = linreg.predict(x_train)\n",
    "y_test_pred = linreg.predict(x_test)\n",
    "\n",
    "# predicted_test_score_improvement = year_over_year_metrics_averaged[[\"club\",\"diff_overall\"]]\n",
    "club_train_set[\"predicted\"] = y_train_pred\n",
    "club_test_set[\"predicted\"] = y_test_pred\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "\n",
    "print (\"Basic Linear Regression\\n\")\n",
    "print (f\"Train MSE basic Linear Regression: {mse_train:.4f}\")\n",
    "print (f\"Test MSE basic Linear Regression: {mse_test:.4f}\")\n",
    "\n",
    "# accuracy_train = accuracy_score(y_train.values, y_train_pred)\n",
    "# accuracy_test = accuracy_score(y_test.values, y_test_pred)\n",
    "# print (f\"Train Accuracy basic Linear Regression: {accuracy_train:.4f}\")\n",
    "# print (f\"Test Accuracy basic Linear Regression: {accuracy_train:.4f}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print (f\"R2 values of Basic Linear Reqression: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Regression Basic Model - Score Calculation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# club_test_set[\"score\"] =  club_test_set[\"diff_overall\"] + (club_test_set[\"diff_overall\"] - club_test_set[\"predicted\"])\n",
    "\n",
    "# club_test_set[['score']] = MinMaxScaler().fit_transform(club_test_set[['score']])\n",
    "# club_test_set['score'] = club_test_set['score'] * 100\n",
    "# club_test_set\n",
    "club_train_set_lin_reg_results, club_test_set_lin_reg_results = generate_scored_with_approach_selection_df(club_train_set, club_test_set)\n",
    "# club_test_set_lin_reg_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayModelTestScoreAgeScatter(club_test_set_lin_reg_results)\n",
    "displayModelTestScoreClubHistogram(club_test_set_lin_reg_results)\n",
    "displayBestClubsOverAllAges(club_test_set_lin_reg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Model Ridge Regularization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set, club_test_set, x_train, y_train, x_test, y_test = train_test_split_custom()\n",
    "\n",
    "maxdeg = 2\n",
    "x_poly_train = PolynomialFeatures(maxdeg).fit_transform(x_train)\n",
    "x_poly_test = PolynomialFeatures(maxdeg).fit_transform(x_test)\n",
    "alpha_list = [0.001, 0.01, 0.1]\n",
    "best_parameter = 0.01\n",
    "# # Create two lists for training and validation error\n",
    "# training_error, validation_error = [],[]\n",
    "# for i in alpha_list:\n",
    "\n",
    "#     print (i)\n",
    "#     ridge_reg = Ridge(alpha=i,normalize=True)\n",
    "\n",
    "#     #Fit on the entire data because we just want to see the trend of the coefficients\n",
    "\n",
    "#     ridge_reg.fit(x_poly_train, y_train)\n",
    "    \n",
    "#     # Perform cross validation on the modified data with neg_mean_squared_error as the scoring parameter and cv=5\n",
    "#     # Remember to get the train_score\n",
    "#     ridge_cv = cross_validate(ridge_reg, x_poly_train, y_train, cv=5,scoring='neg_mean_squared_error',return_train_score=True)\n",
    "\n",
    "#     # Compute the training and validation errors got after cross validation\n",
    "#     mse_train = np.mean(np.abs(ridge_cv[\"train_score\"]))\n",
    "#     mse_val = np.mean(np.abs(ridge_cv[\"test_score\"]))\n",
    "\n",
    "#     # Append the MSEs to their respective lists \n",
    "#     training_error.append(mse_train)\n",
    "#     validation_error.append(mse_val)\n",
    "    \n",
    "#     print(f\"done {i}\")\n",
    "    \n",
    "# # Get the best mse from the validation_error list\n",
    "# best_mse  =  min(validation_error)\n",
    "\n",
    "# # Get the best alpha value based on the best mse\n",
    "# best_parameter = alpha_list[validation_error.index(best_mse)]\n",
    "# print (best_parameter)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg = Ridge(alpha=best_parameter,normalize=True)\n",
    "\n",
    "#Fit on the entire data because we just want to see the trend of the coefficients\n",
    "\n",
    "ridge_reg.fit(x_poly_train, y_train)\n",
    "\n",
    "y_train_pred = ridge_reg.predict(x_poly_train)\n",
    "y_test_pred = ridge_reg.predict(x_poly_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_test_score_improvement = year_over_year_metrics_averaged[[\"club\",\"diff_overall\"]]\n",
    "\n",
    "club_train_set[\"predicted\"] = y_train_pred\n",
    "club_test_set[\"predicted\"] = y_test_pred\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print (f\"Train MSE for Ridge Model {mse_train:.4f}\")\n",
    "print (f\"Test MSE for Ridge Model {mse_test:.4f}\")\n",
    "\n",
    "\n",
    "# accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "# accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "# print (f\"Train Accuracy Ridge Model: {accuracy_train:.4f}\")\n",
    "# print (f\"Test Accuracy Ridge Model: {accuracy_train:.4f}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print (f\"R2 for Ridge Model: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Ridge Model - Score Calculation and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set_ridge_reg_results, club_test_set_ridge_reg_results = generate_scored_with_approach_selection_df(club_train_set, club_test_set)\n",
    "club_train_set_ridge_reg_results.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayModelTestScoreAgeScatter(club_test_set_ridge_reg_results)\n",
    "displayModelTestScoreClubHistogram(club_test_set_ridge_reg_results)\n",
    "displayBestClubsOverAllAges(club_test_set_ridge_reg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set, club_test_set, x_train, y_train, x_test, y_test = train_test_split_custom()\n",
    "\n",
    "max_depth = 10\n",
    "random_state = 144\n",
    "random_forest = RandomForestRegressor(max_depth=max_depth, random_state=random_state, n_estimators=250, max_features = 0.5)\n",
    "\n",
    "# Fit the model on the training set\n",
    "random_forest.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = random_forest.predict(x_train)\n",
    "y_test_pred = random_forest.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted_test_score_improvement = year_over_year_metrics_averaged[[\"club\",\"diff_overall\"]]\n",
    "club_train_set[\"predicted\"] = y_train_pred\n",
    "club_test_set[\"predicted\"] = y_test_pred\n",
    "\n",
    "mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "print (f\"Train MSE for Random Forest {mse_train:.4f}\")\n",
    "print (f\"Test MSE for Random Forest {mse_test:.4f}\")\n",
    "\n",
    "# accuracy_train = accuracy_score(y_train, y_train_pred)\n",
    "# accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "# print (f\"Train Accuracy Random Forest Model: {accuracy_train:.4f}\")\n",
    "# print (f\"Test Accuracy Random Forest Model: {accuracy_train:.4f}\")\n",
    "\n",
    "r2 = r2_score(y_test, y_test_pred)\n",
    "print (f\"R2 Squared value for Random Forest: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Scoring Approach Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scoring Analysis\n",
    "\n",
    "After selecting the Random Forest as the model, we now plan to evaluate the scoring approaches and determine which is the one we like the most. \n",
    "\n",
    "We are going to use the scores to determine the clubs which have done.\n",
    "The graphs generated are:\n",
    "1. Scores vs Age which identifies what is the club distribution and the scores for clubs for Age.\n",
    "2. Histogram of the scores\n",
    "3. A Table of Top 10 Clubs for each player age available in the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Difference Overall Plus Difference in Prediction Scoring Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set_rf_results, club_test_set_rf_results = generate_scored_with_approach_selection_df(club_train_set, club_test_set)\n",
    "club_train_set_rf_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayModelTestScoreAgeScatter(club_test_set_rf_results)\n",
    "displayModelTestScoreClubHistogram(club_test_set_rf_results)\n",
    "displayBestClubsOverAllAges(club_test_set_rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "This is the selected scoring approach and is set as default for the score generator and was also validated using some manual validation and using SME.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Overall Plus Difference in Prediction Scoring Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set_rf_results, club_test_set_rf_results = generate_scored_with_approach_selection_df(club_train_set, club_test_set, ScoreApproach.OVERALL_PLUS_DIFF_OVERALL_PLUS_DIFFERENCE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayModelTestScoreAgeScatter(club_test_set_rf_results)\n",
    "displayModelTestScoreClubHistogram(club_test_set_rf_results)\n",
    "displayBestClubsOverAllAges(club_test_set_rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "This clearly resulted in clubs who were the most popular clubs with great players. However did not reflect if it had helped improve the players because the absolute value of the overall score was being considered. We hence felt the difference was important.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Diff Overall Times Difference in Prediction Scoring Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set_rf_results, club_test_set_rf_results = generate_scored_with_approach_selection_df(club_train_set, club_test_set, ScoreApproach.DIFF_OVERALL_TIMES_DIFFERENCE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayModelTestScoreAgeScatter(club_test_set_rf_results)\n",
    "displayModelTestScoreClubHistogram(club_test_set_rf_results)\n",
    "displayBestClubsOverAllAges(club_test_set_rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "The multiplication emphasized the importance of the Diff Overall and hence was rejected.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest - Difference in Prediction Scoring Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set_rf_results, club_test_set_rf_results = generate_scored_with_approach_selection_df(club_train_set, club_test_set, ScoreApproach.DIFF_OVERALL_PREDICTED)\n",
    "club_train_set_rf_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displayModelTestScoreAgeScatter(club_test_set_rf_results)\n",
    "displayModelTestScoreClubHistogram(club_test_set_rf_results)\n",
    "displayBestClubsOverAllAges(club_test_set_rf_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "This approach heavily relied on the prediction of the model and could result in identifying clubs which if were incorrectly predicted would result in giving significant advantage to the clubs. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Best Clubs Overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_train_set_rf_results, club_test_set_rf_results = generate_scored_with_approach_selection_df(club_train_set, club_test_set)\n",
    "displayBestClubsOverAllAges(club_test_set_rf_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "club_test_set_model = club_test_set_rf_results.drop_duplicates()\n",
    "all_ages_club_mean_scores_df = club_test_set_model[[\"club\", \"diff_overall\", \"predicted\",  \"score_raw\",  \"score\"]]\n",
    "result_df = all_ages_club_mean_scores_df.sort_values(by=\"score\", ascending=False)\n",
    "result_df.to_csv('./data/ordered_clubs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## END OF SUBMISSION\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
